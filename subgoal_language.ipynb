{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:08:25.427041Z",
     "start_time": "2025-03-31T23:08:22.823503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time"
   ],
   "id": "44b264f859e1429e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-31 16:08:24 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:09:20.486602Z",
     "start_time": "2025-03-31T23:08:31.943890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = LLM(\n",
    "    # model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    dtype=\"bfloat16\",\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=1024\n",
    ")\n",
    "\n",
    "# Configure generation parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=256  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# # Configure generation parameters\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature=1.0,\n",
    "#     top_p=0.998,\n",
    "#     max_tokens=256  # Adjust based on your needs\n",
    "# )\n",
    "\n"
   ],
   "id": "a4bd55f97d9b565c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-31 16:08:36 [config.py:585] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-31 16:08:36 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 03-31 16:08:38 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='google/gemma-3-4b-it', speculative_config=None, tokenizer='google/gemma-3-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=google/gemma-3-4b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 16:08:38,435 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-31 16:08:38 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x72dd5fc6d910>\n",
      "INFO 03-31 16:08:38 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-31 16:08:38 [cuda.py:220] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-31 16:08:43 [gpu_model_runner.py:1174] Starting to load model google/gemma-3-4b-it...\n",
      "INFO 03-31 16:08:43 [config.py:3243] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]\n",
      "INFO 03-31 16:08:43 [topk_topp_sampler.py:53] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 03-31 16:08:43 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dbabaf9bda24f0ab371d7a9f8b8eda3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-31 16:08:45 [loader.py:447] Loading weights took 1.10 seconds\n",
      "INFO 03-31 16:08:45 [gpu_model_runner.py:1186] Model loading took 8.5833 GB and 1.587611 seconds\n",
      "INFO 03-31 16:08:45 [gpu_model_runner.py:1456] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.\n",
      "INFO 03-31 16:08:56 [backends.py:415] Using cache directory: /home/asjchoi/.cache/vllm/torch_compile_cache/b2cfc5c0b6/rank_0_0 for vLLM's torch.compile\n",
      "INFO 03-31 16:08:56 [backends.py:425] Dynamo bytecode transform time: 7.37 s\n",
      "INFO 03-31 16:08:57 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 03-31 16:09:03 [monitor.py:33] torch.compile takes 7.37 s in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 16:09:03,875 - INFO - flashinfer.jit: Loading JIT ops: sampling\n",
      "/home/asjchoi/Desktop/Code/UVD/llm-venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/home/asjchoi/Desktop/Code/UVD/llm-venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "2025-03-31 16:09:03,886 - INFO - flashinfer.jit: Finished loading JIT ops: sampling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-31 16:09:04 [kv_cache_utils.py:566] GPU KV cache size: 26,608 tokens\n",
      "INFO 03-31 16:09:04 [kv_cache_utils.py:569] Maximum concurrency for 1,024 tokens per request: 25.98x\n",
      "INFO 03-31 16:09:19 [gpu_model_runner.py:1534] Graph capturing finished in 14 secs, took 0.55 GiB\n",
      "INFO 03-31 16:09:20 [core.py:151] init engine (profile, create kv cache, warmup model) took 35.27 seconds\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T23:09:33.999817Z",
     "start_time": "2025-03-31T23:09:30.536388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wrap_prompt(prompt: str) -> str:\n",
    "    return f'<｜User｜> {prompt} <｜Assistant｜>'\n",
    "\n",
    "with open(\"val_language.json\", \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "\n",
    "example1 = \"put green squash into pot or pan\"  #2\n",
    "example2 = 'Pick the carrot from left side and place it on the orange cloth'  # 3\n",
    "example3 = 'open the drawer'\n",
    "example4 = 'uncover the pot and put the lid on the left side of the cloth'\n",
    "\n",
    "sentences = list(prompts.values())[2210:2220]\n",
    "\n",
    "for lang, num_sg in sentences:\n",
    "    system_prompt = (f\"I need to decompose a language command into 2 smaller language commands. \"\n",
    "                     f\"This language command consists of grasping an object and placing it somewhere. \"\n",
    "                     f\"Here are two examples. \"\n",
    "                     f\"Example 1: Decompose '{example1}' into 2 smaller language commands. Output format:\\n\"\n",
    "                     f\"- grasp the green squash.\\n\"\n",
    "                     f\"- place the green squash into the pot or pan.\\n\"\n",
    "                     f\"Example 2: Decompose '{example2}' into 2 smaller language commands. Output format:\\n\"\n",
    "                     f\"- grab the carrot from the left side.\\n\"\n",
    "                     f\"- place the carrot on the orange cloth.\\n\"\n",
    "                     f\"Example 3: Decompose '{example3}' into 2 smaller language commands. Output format:\\n\"\n",
    "                     f\"- grasp the drawer handling.\\n\"\n",
    "                     f\"- pull the drawer open.\\n\"\n",
    "                     f\"Example 4: Decompose '{example4}' into 2 smaller language commands. Output format:\\n\"\n",
    "                     f\"- grab the pot lid.\\n\"\n",
    "                     f\"- put the pot lid on the left side of the cloth.\\n\"\n",
    "                     f\"Now decompose this language command: '{lang}' into exactly 2 smaller language commands. \"\n",
    "                     f\"For each commands verb, here are the last 5 you used. Avoid using the same ones. \"\n",
    "                     f\"Already used\n",
    "                     f\"IMPORTANT: You must ONLY output the 2 bullet points with no additional text, explanations, or thinking process. \"\n",
    "                     f\"Start each command with a dash and a space.\")\n",
    "\n",
    "    # Approximate\n",
    "    num_tokens = len(system_prompt.split())\n",
    "    assert num_tokens < 1024\n",
    "\n",
    "    system_prompt = wrap_prompt(system_prompt)\n",
    "    #\n",
    "    outputs = llm.generate([system_prompt], sampling_params)\n",
    "\n",
    "    # Extract and store the generated text\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    print(lang)\n",
    "    print(generated_text)\n",
    "\n",
    "# results.append({\n",
    "#     \"prompt_id\": i,\n",
    "#     \"prompt\": prompt,\n",
    "#     \"generated_text\": generated_text\n",
    "# })\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9fc4116b1be0132d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s, est. speed input: 774.69 toks/s, output: 48.25 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take the lid and covered it to the silver pot\n",
      "\n",
      "- grasp the lid.\n",
      "- cover the lid to the silver pot.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.80it/s, est. speed input: 805.06 toks/s, output: 56.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put the purple towel in the washing machine\n",
      "\n",
      "- grasp the purple towel.\n",
      "- place the purple towel in the washing machine.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.78it/s, est. speed input: 787.14 toks/s, output: 47.28 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put sushi on plate\n",
      "\n",
      "- grasp the sushi.\n",
      "- place the sushi on the plate.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s, est. speed input: 636.64 toks/s, output: 50.49 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move the blue spoon on top of the rightburner switch\n",
      "\n",
      "- grasp the blue spoon.\n",
      "- place the blue spoon on top of the rightburner switch.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s, est. speed input: 832.54 toks/s, output: 55.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put the stuffed toy inside the drawer\n",
      "\n",
      "- grasp the stuffed toy.\n",
      "- place the stuffed toy inside the drawer.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s, est. speed input: 1018.34 toks/s, output: 54.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open cabinet\n",
      "\n",
      "- grasp the cabinet door.\n",
      "- open the cabinet.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s, est. speed input: 905.83 toks/s, output: 57.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take broccoli out of pan\n",
      "\n",
      "- grasp the broccoli.\n",
      "- take the broccoli out of the pan.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s, est. speed input: 933.87 toks/s, output: 55.70 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put the spoon on the table\n",
      "\n",
      "- grasp the spoon.\n",
      "- place the spoon on the table.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s, est. speed input: 1080.51 toks/s, output: 56.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take the silver lead and close the pot\n",
      "\n",
      "- grasp the silver lead.\n",
      "- close the pot.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s, est. speed input: 752.19 toks/s, output: 57.26 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move the yellow cloth to the centre of the table\n",
      "\n",
      "- grasp the yellow cloth.\n",
      "- place the yellow cloth in the centre of the table.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
